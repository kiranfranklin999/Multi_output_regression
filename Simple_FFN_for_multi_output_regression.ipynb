{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning and neural networks: \n",
    "\n",
    "* Deep learning is a subfield of machine learning that focuses on neural networks with many layers. \n",
    "* A neural network is a computational model inspired by the way the human brain processes information. It consists of interconnected nodes (neurons) organized in layers. Each neuron receives input from previous neurons, processes it, and passes the output to the next neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.datasets import make_regression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code imports the required libraries: numpy for numerical operations, sklearn for generating a regression dataset, and keras for building and training a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=3, random_state=2)\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_dataset() function generates a regression dataset using make_regression() from sklearn. It creates 1000 samples with 10 features, out of which 5 are informative. There are 3 target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(n_outputs, kernel_initializer='he_uniform'))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_model() function defines a sequential neural network model using Sequential() from keras. It has a single hidden layer with 20 neurons, which uses the ReLU activation function. The output layer has n_outputs neurons. The model is compiled with the mean absolute error (MAE) loss function and the Adam optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural network: \n",
    "The code you provided implements a feedforward neural network, which means that the information flows in one direction from the input layer to the output layer without any loops. In this case, the network has two layers: an input (hidden) layer and an output layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions:\n",
    "Neurons in a neural network use activation functions to introduce non-linearity into the model. In this case, the input layer uses the ReLU (Rectified Linear Unit) activation function, which is defined as ReLU(x) = max(0, x). This means that if the input is positive, the function returns the input value, and if the input is negative, it returns 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimization: \n",
    "To train a neural network, we need a loss function that measures the difference between the predicted output and the actual output (ground truth). In this case, the Mean Absolute Error (MAE) is used as the loss function. The goal of training is to minimize the loss function by adjusting the weights and biases of the network. The Adam optimizer is used in this code to update the weights and biases during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's revisit the mathematical representation of the neural network:\n",
    "\n",
    "Input layer: `h1 = ReLU(W1 * x + b1)`\n",
    "Output layer: `y_pred = W2 * h1 + b2`\n",
    "\n",
    "Here, x is the input feature vector, W1 and W2 are the weight matrices, b1 and b2 are the bias vectors, and h1 is the output of the first layer (ReLU activation).\n",
    "During training, the weights and biases (W1, W2, b1, and b2) are updated to minimize the loss function (MAE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x167e9524820>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "X, y = get_dataset()\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "# get model\n",
    "model = get_model(n_inputs, n_outputs)\n",
    "# fit the model on all data\n",
    "model.fit(X, y, verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 339ms/step\n",
      "Predicted: [-142.8951    -83.09769   -92.076965]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for new data\n",
    "row = [-0.99859353,2.19284309,-0.42632569,-0.21043258,-1.13655612,-0.55671602,-0.63169045,-0.87625098,-0.99445578,-0.3677487]\n",
    "newX = asarray([row])\n",
    "yhat = model.predict(newX)\n",
    "print('Predicted: %s' % yhat[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, you can use it to make predictions for new input samples using the same mathematical equation:\n",
    "\n",
    "- `yhat = W2 * ReLU(W1 * x + b1) + b2`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Above equation we get know how the prediction happens mathematically. But how excatly this equation helps in giving 3 outputs for single set of features? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output layer of the neural network has a number of neurons equal to the number of target variables (3 in this case). \n",
    "\n",
    "- Each neuron in the output layer is responsible for predicting one of the target variables. The weight matrix W2 and bias vector b2 in the output layer are adjusted accordingly to accommodate multiple outputs.\n",
    "- Let's denote the output layer's weight matrix as W2 with dimensions (20, 3) and the bias vector b2 with dimensions (1, 3). The output of the first layer, h1, has dimensions (1, 20). \n",
    "- The mathematical equation for the output layer becomes:\n",
    "\n",
    "    `y_pred = h1 * W2 + b2`\n",
    "\n",
    "Here, y_pred is a (1, 3) matrix, where each element corresponds to the prediction of one of the target variables.\n",
    "\n",
    "- To break it down further, let's represent W2 as three column vectors w2_1, w2_2, and w2_3, and b2 as three scalar values b2_1, b2_2, and b2_3. The equation for the output layer can be rewritten as:\n",
    "\n",
    "    `y_pred = [h1 * w2_1 + b2_1, h1 * w2_2 + b2_2, h1 * w2_3 + b2_3]`\n",
    "\n",
    "- Each element in the y_pred vector represents the prediction for a specific target variable. The neural network learns to adjust the weights and biases during training to minimize the loss function (MAE) for all target variables simultaneously.\n",
    "\n",
    "In summary, multi-output prediction in this neural network is achieved by having multiple neurons in the output layer, each responsible for predicting one target variable. The weight matrix and bias vector in the output layer are adjusted to accommodate multiple outputs, and the final prediction is a vector containing the predicted values for all target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3.02841210e+00, -2.93569851e+00,  2.81300354e+00,\n",
       "          3.35917091e+00, -2.15088391e+00,  2.52842236e+00,\n",
       "         -1.66626668e+00, -3.49653006e+00, -2.04469609e+00,\n",
       "          2.97652483e+00, -3.07689285e+00,  2.19050264e+00,\n",
       "          1.33807743e+00, -3.60908389e+00, -2.94842243e+00,\n",
       "          2.95717764e+00,  3.30104089e+00,  7.15663493e-01,\n",
       "         -1.08380914e+00,  2.44521070e+00],\n",
       "        [ 1.25329703e-01, -2.04313368e-01,  2.18125079e-02,\n",
       "         -7.02505708e-02,  6.52799785e-01, -6.97129190e-01,\n",
       "         -1.88534975e-01, -4.38837614e-03, -8.32859874e-01,\n",
       "          2.76842743e-01,  2.95069158e-01, -2.70908892e-01,\n",
       "         -2.51155883e-01,  7.41884634e-02, -1.65855169e-01,\n",
       "          4.29592788e-01,  1.80095017e-01,  9.38812047e-02,\n",
       "          9.68291983e-02, -3.90892267e-01],\n",
       "        [ 1.23813558e+00,  4.15536389e-02,  7.50992894e-01,\n",
       "          2.16997519e-01, -2.81887919e-01,  9.49331284e-01,\n",
       "         -1.35934126e+00, -1.28497660e+00, -1.61139274e+00,\n",
       "          3.50670189e-01, -1.69773734e+00,  9.65979636e-01,\n",
       "         -2.30133608e-01, -1.97687721e+00,  1.11000940e-01,\n",
       "          1.04147351e+00,  6.28462434e-01,  1.46825695e+00,\n",
       "          7.95021653e-01, -1.08986221e-01],\n",
       "        [-5.18607140e-01, -7.52719223e-01,  4.57102433e-02,\n",
       "          3.73740584e-01, -2.34634995e+00,  1.67754936e+00,\n",
       "          1.24376738e+00,  1.62775129e-01, -1.80915022e+00,\n",
       "          9.41041231e-01,  5.92055023e-01, -1.39830792e+00,\n",
       "          3.10160446e+00, -6.45314455e-01, -2.02247024e+00,\n",
       "          1.56084752e+00,  7.95595050e-01, -5.94966471e-01,\n",
       "         -5.62478781e-01,  3.50404352e-01],\n",
       "        [-3.51158947e-01,  5.52355289e-01, -1.37671083e-01,\n",
       "          9.18307900e-02, -6.72273934e-01, -2.96227392e-02,\n",
       "         -2.16776520e-01,  2.49377415e-01,  8.92668188e-01,\n",
       "          2.93546885e-01, -4.37484503e-01, -8.57749656e-02,\n",
       "         -1.81133002e-01, -3.83531511e-01,  9.28491205e-02,\n",
       "          5.05414486e-01, -3.01284194e-01,  1.75386995e-01,\n",
       "         -1.41964927e-01,  5.24752796e-01],\n",
       "        [ 6.86436296e-02,  4.76720124e-01, -3.50583196e-01,\n",
       "          1.87053114e-01,  9.62843895e-01,  3.61561328e-01,\n",
       "          3.16637568e-02,  4.80693549e-01,  5.05518734e-01,\n",
       "          2.40086555e-01, -8.66306759e-03,  4.43112910e-01,\n",
       "         -3.76626164e-01, -7.38229215e-01, -8.21552217e-01,\n",
       "          4.83089000e-01, -3.88835222e-01, -9.75869223e-02,\n",
       "         -7.11698309e-02,  3.96241426e-01],\n",
       "        [ 1.49025738e-01,  6.61552072e-01, -5.96554875e-01,\n",
       "         -1.36935055e-01,  4.35802341e-01, -3.39466155e-01,\n",
       "          2.07681835e-01, -1.23397045e-01, -5.29602826e-01,\n",
       "          1.46505684e-01, -4.92311008e-02,  4.67984945e-01,\n",
       "          1.98264141e-02, -7.49221981e-01, -4.68195528e-02,\n",
       "          4.91573624e-02,  2.95418173e-01, -4.15346533e-01,\n",
       "          5.41800298e-02,  1.34745790e-02],\n",
       "        [ 1.58883870e+00, -1.48351026e+00,  1.70065820e+00,\n",
       "         -1.68250155e+00, -7.83678591e-02,  1.33285791e-01,\n",
       "         -3.26805663e+00, -1.13290644e+00,  3.84984761e-01,\n",
       "          2.64751256e-01, -2.55078578e+00,  2.28733087e+00,\n",
       "         -2.09363014e-01, -2.01210284e+00,  9.51156437e-01,\n",
       "          6.80338681e-01,  1.81006432e+00,  3.33549404e+00,\n",
       "          2.17836142e+00,  9.86091569e-02],\n",
       "        [ 4.20243276e-04, -2.73008198e-01, -4.06129330e-01,\n",
       "         -8.01412538e-02,  3.49270314e-01, -1.94833219e-01,\n",
       "          2.16286108e-01,  2.75285304e-01, -2.15994909e-01,\n",
       "         -2.51329750e-01, -6.12200536e-02,  3.30884419e-02,\n",
       "          1.35679811e-01, -3.40343148e-01, -9.06422362e-02,\n",
       "          4.30417210e-02,  5.92751145e-01, -2.24504471e-01,\n",
       "         -2.55006067e-02, -2.80369788e-01],\n",
       "        [ 1.39135015e+00, -9.89343882e-01, -7.68098891e-01,\n",
       "         -4.33741474e+00, -2.30668712e+00,  2.14611936e+00,\n",
       "         -2.25981164e+00,  1.37487686e+00, -2.64336610e+00,\n",
       "          1.12102687e+00, -1.93453479e+00,  1.02403820e+00,\n",
       "          3.20707846e+00, -2.41899681e+00, -1.73661649e+00,\n",
       "          1.28137195e+00, -3.34870398e-01,  2.03091455e+00,\n",
       "          3.87437654e+00,  1.25580823e+00]], dtype=float32),\n",
       " array([1.6186734, 2.3807092, 2.188062 , 3.47544  , 1.8490405, 1.7098098,\n",
       "        2.3495   , 2.7720199, 2.2505655, 1.9499811, 2.4122233, 1.6337802,\n",
       "        2.378255 , 2.4685996, 2.2126381, 1.7216557, 2.144704 , 2.0374973,\n",
       "        3.021613 , 1.7945126], dtype=float32),\n",
       " array([[ 2.6677027 ,  1.4864898 ,  1.0607805 ],\n",
       "        [-3.4222345 , -1.3631976 , -2.2821295 ],\n",
       "        [ 3.4159176 ,  0.10158447,  1.0519166 ],\n",
       "        [ 2.676397  , -3.51627   ,  0.90005547],\n",
       "        [-1.4175166 , -1.1777086 , -2.5825603 ],\n",
       "        [ 1.4182163 ,  1.0702401 ,  2.2716985 ],\n",
       "        [-2.6135817 , -2.6197908 , -0.18914604],\n",
       "        [-3.483523  ,  0.9703418 , -0.8592545 ],\n",
       "        [-1.5906218 , -1.2890077 , -2.6062531 ],\n",
       "        [ 2.479437  ,  0.7127459 ,  2.383226  ],\n",
       "        [-2.9848113 , -2.1456244 , -1.1008481 ],\n",
       "        [ 2.7254212 ,  2.1320827 , -0.13794497],\n",
       "        [-0.11634828,  1.7799549 ,  3.1383176 ],\n",
       "        [-2.7893364 , -1.5704194 , -1.6063287 ],\n",
       "        [-1.8259448 , -0.7502737 , -3.3590302 ],\n",
       "        [ 2.2603314 ,  0.7931277 ,  2.306257  ],\n",
       "        [ 2.7416463 ,  0.17844918,  1.2582028 ],\n",
       "        [ 1.7244071 ,  2.7538815 , -0.1633799 ],\n",
       "        [-0.56236345,  3.825541  ,  0.04879535],\n",
       "        [ 2.061264  ,  0.8523581 ,  2.076617  ]], dtype=float32),\n",
       " array([-0.8876219,  0.5400527, -0.8150125], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, Are you wondering how exactly Loss function and optimizer comes in picture?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function: \n",
    "The loss function measures the difference between the predicted output and the actual output (ground truth). It quantifies how well the neural network is performing. \n",
    "- In this case, the Mean Absolute Error (MAE) is used as the loss function. The MAE is calculated as the average of the absolute differences between the predicted and actual values for each target variable.\n",
    "- Mathematically, for a single data point, the MAE is defined as:\n",
    "\n",
    "    `MAE = (1/n) * Î£|y_pred_i - y_true_i|`\n",
    "    \n",
    "where n is the number of target variables, y_pred_i is the predicted value for the i-th target variable, and y_true_i is the actual value for the i-th target variable.\n",
    "\n",
    "#### Optimizer: \n",
    "The optimizer is responsible for updating the weights and biases of the neural network to minimize the loss function. It determines how the model learns from the data. \n",
    "- In this case, the Adam optimizer is used. Adam is an adaptive learning rate optimization algorithm that combines the advantages of two other popular optimizers, AdaGrad and RMSProp. It adjusts the learning rate for each weight and bias individually, making it suitable for a wide range of problems.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### To summarize the overall training process:\n",
    "\n",
    "1. Initialize the neural network with random weights and biases.\n",
    "\n",
    "2.  For each epoch (iteration over the entire dataset): \n",
    "\n",
    "    a. Forward pass: \n",
    "\n",
    "        Calculate the predicted output y_pred for each input sample using the current weights and biases. \n",
    "\n",
    "    b. Calculate the loss: \n",
    "    \n",
    "        Compute the MAE between the predicted output y_pred and the actual output y_true. \n",
    "    \n",
    "    c. Backward pass: \n",
    "        Compute the gradients of the loss function with respect to the weights and biases using backpropagation. \n",
    "    \n",
    "    d. Update the weights and biases using the Adam optimizer, which adjusts them based on the computed gradients and its adaptive learning rate.\n",
    "\n",
    "Repeat steps 2a-2d for the specified number of epochs or until the loss converges.\n",
    "\n",
    "After training, the neural network can be used to make predictions for new input samples using the learned weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
